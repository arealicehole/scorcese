all documenttation found here :https://openai.github.io/openai-agents-python

Skip to content
logo
OpenAI Agents SDK
Intro

    English
    日本語
    한국어
    简体中文

Type to start searching
openai-agents-python

    v0.6.7
    18.4k
    3.1k

OpenAI Agents SDK

    Intro
    Quickstart
    Examples
    Documentation
        Agents
        Running agents
        Sessions
            Sessions
            SQLAlchemy Sessions
            Advanced SQLite Sessions
            Encrypted Sessions
        Results
        Streaming
        REPL utility
        Tools
        Model context protocol (MCP)
        Handoffs
        Tracing
        Context management
        Guardrails
        Orchestrating multiple agents
        Usage
        Models
            Models
            Using any model via LiteLLM
        Configuring the SDK
        Agent Visualization
        Release process/changelog
        Voice agents
            Quickstart
            Pipelines and workflows
            Tracing
        Realtime agents
            Quickstart
            Guide
    API Reference
        Agents
            Agents module
            Agents
            Runner
            Memory
            repl
            Tools
            Tool Context
            Results
            Streaming events
            Handoffs
            Lifecycle
            Items
            Run context
            Usage
            Exceptions
            Guardrails
            Prompts
            Model settings
            Strict Schema
            Tool Guardrails
            Computer
            Agent output
            Function schema
            Model interface
            OpenAI Chat Completions model
            OpenAI Responses model
            MCP Servers
            MCP Util
        Tracing
            Tracing module
            Creating traces/spans
            Traces
            Spans
            Processor interface
            Processors
            Scope
            Setup
            Span data
            Util
        Realtime
            RealtimeAgent
            RealtimeRunner
            RealtimeSession
            Realtime Events
            Realtime Configuration
            Model
        Voice
            Pipeline
            Workflow
            Input
            Result
            Pipeline Config
            Events
            Exceptions
            Model
            Utils
            OpenAIVoiceModelProvider
            OpenAI STT
            OpenAI TTS
        Extensions
            Handoff filters
            Handoff prompt
            LiteLLM Models
            SQLAlchemySession
            EncryptedSession
            AdvancedSQLiteSession

Table of contents

    Why use the Agents SDK
    Installation
    Hello world example

OpenAI Agents SDK

The OpenAI Agents SDK enables you to build agentic AI apps in a lightweight, easy-to-use package with very few abstractions. It's a production-ready upgrade of our previous experimentation for agents, Swarm. The Agents SDK has a very small set of primitives:

    Agents, which are LLMs equipped with instructions and tools
    Handoffs, which allow agents to delegate to other agents for specific tasks
    Guardrails, which enable validation of agent inputs and outputs
    Sessions, which automatically maintains conversation history across agent runs

In combination with Python, these primitives are powerful enough to express complex relationships between tools and agents, and allow you to build real-world applications without a steep learning curve. In addition, the SDK comes with built-in tracing that lets you visualize and debug your agentic flows, as well as evaluate them and even fine-tune models for your application.
Why use the Agents SDK

The SDK has two driving design principles:

    Enough features to be worth using, but few enough primitives to make it quick to learn.
    Works great out of the box, but you can customize exactly what happens.

Here are the main features of the SDK:

    Agent loop: Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done.
    Python-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.
    Handoffs: A powerful feature to coordinate and delegate between multiple agents.
    Guardrails: Run input validations and checks in parallel to your agents, breaking early if the checks fail.
    Sessions: Automatic conversation history management across agent runs, eliminating manual state handling.
    Function tools: Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation.
    Tracing: Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools.

Installation

pip
 install openai-agents

Hello world example

from agents import Agent, Runner

agent = Agent(name="Assistant", instructions="You are a helpful assistant")

result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
print(result.final_output)

# Code within the code,
# Functions calling themselves,
# Infinite loop's dance.

(If running this, ensure you set the OPENAI_API_KEY environment variable)

export OPENAI_API_KEY=sk-...


Skip to content
logo
OpenAI Agents SDK
Quickstart

    English
    日本語
    한국어
    简体中文

Type to start searching
openai-agents-python

    v0.6.7
    18.4k
    3.1k

OpenAI Agents SDK

    Intro
    Quickstart
    Examples
    Documentation
        Agents
        Running agents
        Sessions
            Sessions
            SQLAlchemy Sessions
            Advanced SQLite Sessions
            Encrypted Sessions
        Results
        Streaming
        REPL utility
        Tools
        Model context protocol (MCP)
        Handoffs
        Tracing
        Context management
        Guardrails
        Orchestrating multiple agents
        Usage
        Models
            Models
            Using any model via LiteLLM
        Configuring the SDK
        Agent Visualization
        Release process/changelog
        Voice agents
            Quickstart
            Pipelines and workflows
            Tracing
        Realtime agents
            Quickstart
            Guide
    API Reference
        Agents
            Agents module
            Agents
            Runner
            Memory
            repl
            Tools
            Tool Context
            Results
            Streaming events
            Handoffs
            Lifecycle
            Items
            Run context
            Usage
            Exceptions
            Guardrails
            Prompts
            Model settings
            Strict Schema
            Tool Guardrails
            Computer
            Agent output
            Function schema
            Model interface
            OpenAI Chat Completions model
            OpenAI Responses model
            MCP Servers
            MCP Util
        Tracing
            Tracing module
            Creating traces/spans
            Traces
            Spans
            Processor interface
            Processors
            Scope
            Setup
            Span data
            Util
        Realtime
            RealtimeAgent
            RealtimeRunner
            RealtimeSession
            Realtime Events
            Realtime Configuration
            Model
        Voice
            Pipeline
            Workflow
            Input
            Result
            Pipeline Config
            Events
            Exceptions
            Model
            Utils
            OpenAIVoiceModelProvider
            OpenAI STT
            OpenAI TTS
        Extensions
            Handoff filters
            Handoff prompt
            LiteLLM Models
            SQLAlchemySession
            EncryptedSession
            AdvancedSQLiteSession

Table of contents

    Create a project and virtual environment
        Activate the virtual environment
        Install the Agents SDK
        Set an OpenAI API key
    Create your first agent
    Add a few more agents
    Define your handoffs
    Run the agent orchestration
    Add a guardrail
    Put it all together
    View your traces
    Next steps

Quickstart
Create a project and virtual environment

You'll only need to do this once.

mkdir
 my_project
cd my_project
python
 -m venv .venv

Activate the virtual environment

Do this every time you start a new terminal session.

source .venv/bin/activate

Install the Agents SDK

pip
 install openai-agents # or `uv add openai-agents`, etc

Set an OpenAI API key

If you don't have one, follow these instructions to create an OpenAI API key.

export OPENAI_API_KEY=sk-...

Create your first agent

Agents are defined with instructions, a name, and optional config (such as model_config)

from agents import Agent

agent = Agent(
    
name="Math Tutor",
    
instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)

Add a few more agents

Additional agents can be defined in the same way. handoff_descriptions provide additional context for determining handoff routing

from agents import Agent

history_tutor_agent = Agent(
    
name="History Tutor",
    
handoff_description="Specialist agent for historical questions",
    
instructions="You provide assistance with historical queries. Explain important events and context clearly.",
)

math_tutor_agent = Agent(
    
name="Math Tutor",
    
handoff_description="Specialist agent for math questions",
    
instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)

Define your handoffs

On each agent, you can define an inventory of outgoing handoff options that the agent can choose from to decide how to make progress on their task.

triage_agent = Agent(
    
name="Triage Agent",
    
instructions="You determine which agent to use based on the user's homework question",
    
handoffs=[history_tutor_agent, math_tutor_agent]
)

Run the agent orchestration

Let's check that the workflow runs and the triage agent correctly routes between the two specialist agents.

from agents import Runner

async def main():
    
result = await Runner.run(triage_agent, "who was the first president of the united states?")
    
print(result.final_output)

Add a guardrail

You can define custom guardrails to run on the input or output.

from agents import GuardrailFunctionOutput, Agent, Runner
from pydantic import BaseModel


class HomeworkOutput(BaseModel):
    
is_homework: bool
    
reasoning: str

guardrail_agent = Agent(
    
name="Guardrail check",
    
instructions="Check if the user is asking about homework.",
    
output_type=HomeworkOutput,
)

async def homework_guardrail(ctx, agent, input_data):
    
result = await Runner.run(guardrail_agent, input_data, context=ctx.context)
    
final_output = result.final_output_as(HomeworkOutput)
    
return GuardrailFunctionOutput(
        
output_info=final_output,
        
tripwire_triggered=not final_output.is_homework,
    
)

Put it all together

Let's put it all together and run the entire workflow, using handoffs and the input guardrail.

from agents import Agent, InputGuardrail, GuardrailFunctionOutput, Runner
from agents.exceptions import InputGuardrailTripwireTriggered
from pydantic import BaseModel
import asyncio

class HomeworkOutput(BaseModel):
    
is_homework: bool
    
reasoning: str

guardrail_agent = Agent(
    
name="Guardrail check",
    
instructions="Check if the user is asking about homework.",
    
output_type=HomeworkOutput,
)

math_tutor_agent = Agent(
    
name="Math Tutor",
    
handoff_description="Specialist agent for math questions",
    
instructions="You provide help with math problems. Explain your reasoning at each step and include examples",
)

history_tutor_agent = Agent(
    
name="History Tutor",
    
handoff_description="Specialist agent for historical questions",
    
instructions="You provide assistance with historical queries. Explain important events and context clearly.",
)


async def homework_guardrail(ctx, agent, input_data):
    
result = await Runner.run(guardrail_agent, input_data, context=ctx.context)
    
final_output = result.final_output_as(HomeworkOutput)
    
return GuardrailFunctionOutput(
        
output_info=final_output,
        
tripwire_triggered=not final_output.is_homework,
    
)

triage_agent = Agent(
    
name="Triage Agent",
    
instructions="You determine which agent to use based on the user's homework question",
    
handoffs=[history_tutor_agent, math_tutor_agent],
    
input_guardrails=[
        
InputGuardrail(guardrail_function=homework_guardrail),
    
],
)

async def main():
    
# Example 1: History question
    
try:
        
result = await Runner.run(triage_agent, "who was the first president of the united states?")
        
print(result.final_output)
    
except InputGuardrailTripwireTriggered as e:
        
print("Guardrail blocked this input:", e)

    
# Example 2: General/philosophical question
    
try:
        
result = await Runner.run(triage_agent, "What is the meaning of life?")
        
print(result.final_output)
    
except InputGuardrailTripwireTriggered as e:
        
print("Guardrail blocked this input:", e)

if __name__ == "__main__":
    
asyncio.run(main())

View your traces

To review what happened during your agent run, navigate to the Trace viewer in the OpenAI Dashboard to view traces of your agent runs.
Next steps

Learn how to build more complex agentic flows:

    Learn about how to configure Agents.
    Learn about running agents.
    Learn about tools, guardrails and models.


Skip to content
logo
OpenAI Agents SDK
Examples

    English
    日本語
    한국어
    简体中文

Type to start searching
openai-agents-python

    v0.6.7
    18.4k
    3.1k

OpenAI Agents SDK

    Intro
    Quickstart
    Examples
    Documentation
        Agents
        Running agents
        Sessions
            Sessions
            SQLAlchemy Sessions
            Advanced SQLite Sessions
            Encrypted Sessions
        Results
        Streaming
        REPL utility
        Tools
        Model context protocol (MCP)
        Handoffs
        Tracing
        Context management
        Guardrails
        Orchestrating multiple agents
        Usage
        Models
            Models
            Using any model via LiteLLM
        Configuring the SDK
        Agent Visualization
        Release process/changelog
        Voice agents
            Quickstart
            Pipelines and workflows
            Tracing
        Realtime agents
            Quickstart
            Guide
    API Reference
        Agents
            Agents module
            Agents
            Runner
            Memory
            repl
            Tools
            Tool Context
            Results
            Streaming events
            Handoffs
            Lifecycle
            Items
            Run context
            Usage
            Exceptions
            Guardrails
            Prompts
            Model settings
            Strict Schema
            Tool Guardrails
            Computer
            Agent output
            Function schema
            Model interface
            OpenAI Chat Completions model
            OpenAI Responses model
            MCP Servers
            MCP Util
        Tracing
            Tracing module
            Creating traces/spans
            Traces
            Spans
            Processor interface
            Processors
            Scope
            Setup
            Span data
            Util
        Realtime
            RealtimeAgent
            RealtimeRunner
            RealtimeSession
            Realtime Events
            Realtime Configuration
            Model
        Voice
            Pipeline
            Workflow
            Input
            Result
            Pipeline Config
            Events
            Exceptions
            Model
            Utils
            OpenAIVoiceModelProvider
            OpenAI STT
            OpenAI TTS
        Extensions
            Handoff filters
            Handoff prompt
            LiteLLM Models
            SQLAlchemySession
            EncryptedSession
            AdvancedSQLiteSession

Table of contents

    Categories

Examples

Check out a variety of sample implementations of the SDK in the examples section of the repo. The examples are organized into several categories that demonstrate different patterns and capabilities.
Categories

    agent_patterns: Examples in this category illustrate common agent design patterns, such as
        Deterministic workflows
        Agents as tools
        Parallel agent execution
        Conditional tool usage
        Input/output guardrails
        LLM as a judge
        Routing
        Streaming guardrails

    basic: These examples showcase foundational capabilities of the SDK, such as
        Hello world examples (Default model, GPT-5, open-weight model)
        Agent lifecycle management
        Dynamic system prompts
        Streaming outputs (text, items, function call args)
        Prompt templates
        File handling (local and remote, images and PDFs)
        Usage tracking
        Non-strict output types
        Previous response ID usage

    customer_service: Example customer service system for an airline.

    financial_research_agent: A financial research agent that demonstrates structured research workflows with agents and tools for financial data analysis.

    handoffs: See practical examples of agent handoffs with message filtering.

    hosted_mcp: Examples demonstrating how to use hosted MCP (Model Context Protocol) connectors and approvals.

    mcp: Learn how to build agents with MCP (Model Context Protocol), including:
        Filesystem examples
        Git examples
        MCP prompt server examples
        SSE (Server-Sent Events) examples
        Streamable HTTP examples

    memory: Examples of different memory implementations for agents, including:
        SQLite session storage
        Advanced SQLite session storage
        Redis session storage
        SQLAlchemy session storage
        Encrypted session storage
        OpenAI session storage

    model_providers: Explore how to use non-OpenAI models with the SDK, including custom providers and LiteLLM integration.

    realtime: Examples showing how to build real-time experiences using the SDK, including:
        Web applications
        Command-line interfaces
        Twilio integration

    reasoning_content: Examples demonstrating how to work with reasoning content and structured outputs.

    research_bot: Simple deep research clone that demonstrates complex multi-agent research workflows.

    tools: Learn how to implement OAI hosted tools such as:
        Web search and web search with filters
        File search
        Code interpreter
        Computer use
        Image generation

    voice: See examples of voice agents, using our TTS and STT models, including streamed voice examples.


Skip to content
logo
OpenAI Agents SDK
Agents

    English
    日本語
    한국어
    简体中文

Type to start searching
openai-agents-python

    v0.6.7
    18.4k
    3.1k

OpenAI Agents SDK

    Intro
    Quickstart
    Examples
    Documentation
        Agents
        Running agents
        Sessions
            Sessions
            SQLAlchemy Sessions
            Advanced SQLite Sessions
            Encrypted Sessions
        Results
        Streaming
        REPL utility
        Tools
        Model context protocol (MCP)
        Handoffs
        Tracing
        Context management
        Guardrails
        Orchestrating multiple agents
        Usage
        Models
            Models
            Using any model via LiteLLM
        Configuring the SDK
        Agent Visualization
        Release process/changelog
        Voice agents
            Quickstart
            Pipelines and workflows
            Tracing
        Realtime agents
            Quickstart
            Guide
    API Reference
        Agents
            Agents module
            Agents
            Runner
            Memory
            repl
            Tools
            Tool Context
            Results
            Streaming events
            Handoffs
            Lifecycle
            Items
            Run context
            Usage
            Exceptions
            Guardrails
            Prompts
            Model settings
            Strict Schema
            Tool Guardrails
            Computer
            Agent output
            Function schema
            Model interface
            OpenAI Chat Completions model
            OpenAI Responses model
            MCP Servers
            MCP Util
        Tracing
            Tracing module
            Creating traces/spans
            Traces
            Spans
            Processor interface
            Processors
            Scope
            Setup
            Span data
            Util
        Realtime
            RealtimeAgent
            RealtimeRunner
            RealtimeSession
            Realtime Events
            Realtime Configuration
            Model
        Voice
            Pipeline
            Workflow
            Input
            Result
            Pipeline Config
            Events
            Exceptions
            Model
            Utils
            OpenAIVoiceModelProvider
            OpenAI STT
            OpenAI TTS
        Extensions
            Handoff filters
            Handoff prompt
            LiteLLM Models
            SQLAlchemySession
            EncryptedSession
            AdvancedSQLiteSession

Table of contents

    Basic configuration
    Context
    Output types
    Multi-agent system design patterns
        Manager (agents as tools)
        Handoffs
    Dynamic instructions
    Lifecycle events (hooks)
    Guardrails
    Cloning/copying agents
    Forcing tool use
    Tool Use Behavior

Agents

Agents are the core building block in your apps. An agent is a large language model (LLM), configured with instructions and tools.
Basic configuration

The most common properties of an agent you'll configure are:

    name: A required string that identifies your agent.
    instructions: also known as a developer message or system prompt.
    model: which LLM to use, and optional model_settings to configure model tuning parameters like temperature, top_p, etc.
    tools: Tools that the agent can use to achieve its tasks.

from agents import Agent, ModelSettings, function_tool

@function_tool
def get_weather(city: str) -> str:
    """returns weather info for the specified city."""
    
return f"The weather in {city} is sunny"

agent = Agent(
    
name="Haiku agent",
    
instructions="Always respond in haiku form",
    
model="gpt-5-nano",
    
tools=[get_weather],
)

Context

Agents are generic on their context type. Context is a dependency-injection tool: it's an object you create and pass to Runner.run(), that is passed to every agent, tool, handoff etc, and it serves as a grab bag of dependencies and state for the agent run. You can provide any Python object as the context.

@dataclass
class UserContext:
    
name: str
    
uid: str
    
is_pro_user: bool

    
async def fetch_purchases() -> list[Purchase]:
        
return ...

agent = Agent[UserContext](
    
...,
)

Output types

By default, agents produce plain text (i.e. str) outputs. If you want the agent to produce a particular type of output, you can use the output_type parameter. A common choice is to use Pydantic objects, but we support any type that can be wrapped in a Pydantic TypeAdapter - dataclasses, lists, TypedDict, etc.

from pydantic import BaseModel
from agents import Agent


class CalendarEvent(BaseModel):
    
name: str
    
date: str
    
participants: list[str]

agent = Agent(
    
name="Calendar extractor",
    
instructions="Extract calendar events from text",
    
output_type=CalendarEvent,
)

Note

When you pass an output_type, that tells the model to use structured outputs instead of regular plain text responses.
Multi-agent system design patterns

There are many ways to design multi‑agent systems, but we commonly see two broadly applicable patterns:

    Manager (agents as tools): A central manager/orchestrator invokes specialized sub‑agents as tools and retains control of the conversation.
    Handoffs: Peer agents hand off control to a specialized agent that takes over the conversation. This is decentralized.

See our practical guide to building agents for more details.
Manager (agents as tools)

The customer_facing_agent handles all user interaction and invokes specialized sub‑agents exposed as tools. Read more in the tools documentation.

from agents import Agent

booking_agent = Agent(...)
refund_agent = Agent(...)

customer_facing_agent = Agent(
    
name="Customer-facing agent",
    
instructions=(
        
"Handle all direct user communication. "
        
"Call the relevant tools when specialized expertise is needed."
    
),
    
tools=[
        
booking_agent.as_tool(
            
tool_name="booking_expert",
            
tool_description="Handles booking questions and requests.",
        
),
        
refund_agent.as_tool(
            
tool_name="refund_expert",
            
tool_description="Handles refund questions and requests.",
        
)
    
],
)

Handoffs

Handoffs are sub‑agents the agent can delegate to. When a handoff occurs, the delegated agent receives the conversation history and takes over the conversation. This pattern enables modular, specialized agents that excel at a single task. Read more in the handoffs documentation.

from agents import Agent

booking_agent = Agent(...)
refund_agent = Agent(...)

triage_agent = Agent(
    
name="Triage agent",
    
instructions=(
        
"Help the user with their questions. "
        
"If they ask about booking, hand off to the booking agent. "
        
"If they ask about refunds, hand off to the refund agent."
    
),
    
handoffs=[booking_agent, refund_agent],
)

Dynamic instructions

In most cases, you can provide instructions when you create the agent. However, you can also provide dynamic instructions via a function. The function will receive the agent and context, and must return the prompt. Both regular and async functions are accepted.

def dynamic_instructions(
    
context: RunContextWrapper[UserContext], agent: Agent[UserContext]
) -> str:
    
return f"The user's name is {context.context.name}. Help them with their questions."


agent = Agent[UserContext](
    
name="Triage agent",
    
instructions=dynamic_instructions,
)

Lifecycle events (hooks)

Sometimes, you want to observe the lifecycle of an agent. For example, you may want to log events, or pre-fetch data when certain events occur. You can hook into the agent lifecycle with the hooks property. Subclass the AgentHooks class, and override the methods you're interested in.
Guardrails

Guardrails allow you to run checks/validations on user input in parallel to the agent running, and on the agent's output once it is produced. For example, you could screen the user's input and agent's output for relevance. Read more in the guardrails documentation.
Cloning/copying agents

By using the clone() method on an agent, you can duplicate an Agent, and optionally change any properties you like.

pirate_agent = Agent(
    
name="Pirate",
    
instructions="Write like a pirate",
    
model="gpt-5.2",
)

robot_agent = pirate_agent.clone(
    
name="Robot",
    
instructions="Write like a robot",
)

Forcing tool use

Supplying a list of tools doesn't always mean the LLM will use a tool. You can force tool use by setting ModelSettings.tool_choice. Valid values are:

    auto, which allows the LLM to decide whether or not to use a tool.
    required, which requires the LLM to use a tool (but it can intelligently decide which tool).
    none, which requires the LLM to not use a tool.
    Setting a specific string e.g. my_tool, which requires the LLM to use that specific tool.

from agents import Agent, Runner, function_tool, ModelSettings

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    
return f"The weather in {city} is sunny"

agent = Agent(
    
name="Weather Agent",
    
instructions="Retrieve weather details.",
    
tools=[get_weather],
    
model_settings=ModelSettings(tool_choice="get_weather")
)

Tool Use Behavior

The tool_use_behavior parameter in the Agent configuration controls how tool outputs are handled:

    "run_llm_again": The default. Tools are run, and the LLM processes the results to produce a final response.
    "stop_on_first_tool": The output of the first tool call is used as the final response, without further LLM processing.

from agents import Agent, Runner, function_tool, ModelSettings

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    
return f"The weather in {city} is sunny"

agent = Agent(
    
name="Weather Agent",
    
instructions="Retrieve weather details.",
    
tools=[get_weather],
    
tool_use_behavior="stop_on_first_tool"
)

    StopAtTools(stop_at_tool_names=[...]): Stops if any specified tool is called, using its output as the final response.

from agents import Agent, Runner, function_tool
from agents.agent import StopAtTools

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    
return f"The weather in {city} is sunny"

@function_tool
def sum_numbers(a: int, b: int) -> int:
    """Adds two numbers."""
    
return a + b

agent = Agent(
    
name="Stop At Stock Agent",
    
instructions="Get weather or sum numbers.",
    
tools=[get_weather, sum_numbers],
    
tool_use_behavior=StopAtTools(stop_at_tool_names=["get_weather"])
)

    ToolsToFinalOutputFunction: A custom function that processes tool results and decides whether to stop or continue with the LLM.

from agents import Agent, Runner, function_tool, FunctionToolResult, RunContextWrapper
from agents.agent import ToolsToFinalOutputResult
from typing import List, Any

@function_tool
def get_weather(city: str) -> str:
    """Returns weather info for the specified city."""
    
return f"The weather in {city} is sunny"

def custom_tool_handler(
    
context: RunContextWrapper[Any],
    
tool_results: List[FunctionToolResult]
) -> ToolsToFinalOutputResult:
    """Processes tool results to decide final output."""
    
for result in tool_results:
        
if result.output and "sunny" in result.output:
            
return ToolsToFinalOutputResult(
                
is_final_output=True,
                
final_output=f"Final weather: {result.output}"
            
)
    
return ToolsToFinalOutputResult(
        
is_final_output=False,
        
final_output=None
    
)

agent = Agent(
    
name="Weather Agent",
    
instructions="Retrieve weather details.",
    
tools=[get_weather],
    
tool_use_behavior=custom_tool_handler
)

Note

To prevent infinite loops, the framework automatically resets tool_choice to "auto" after a tool call. This behavior is configurable via agent.reset_tool_choice. The infinite loop is because tool results are sent to the LLM, which then generates another tool call because of tool_choice, ad infinitum.

Skip to content
logo
OpenAI Agents SDK
Tools

    English
    日本語
    한국어
    简体中文

Type to start searching
openai-agents-python

    v0.6.7
    18.4k
    3.1k

OpenAI Agents SDK

    Intro
    Quickstart
    Examples
    Documentation
        Agents
        Running agents
        Sessions
            Sessions
            SQLAlchemy Sessions
            Advanced SQLite Sessions
            Encrypted Sessions
        Results
        Streaming
        REPL utility
        Tools
        Model context protocol (MCP)
        Handoffs
        Tracing
        Context management
        Guardrails
        Orchestrating multiple agents
        Usage
        Models
            Models
            Using any model via LiteLLM
        Configuring the SDK
        Agent Visualization
        Release process/changelog
        Voice agents
            Quickstart
            Pipelines and workflows
            Tracing
        Realtime agents
            Quickstart
            Guide
    API Reference
        Agents
            Agents module
            Agents
            Runner
            Memory
            repl
            Tools
            Tool Context
            Results
            Streaming events
            Handoffs
            Lifecycle
            Items
            Run context
            Usage
            Exceptions
            Guardrails
            Prompts
            Model settings
            Strict Schema
            Tool Guardrails
            Computer
            Agent output
            Function schema
            Model interface
            OpenAI Chat Completions model
            OpenAI Responses model
            MCP Servers
            MCP Util
        Tracing
            Tracing module
            Creating traces/spans
            Traces
            Spans
            Processor interface
            Processors
            Scope
            Setup
            Span data
            Util
        Realtime
            RealtimeAgent
            RealtimeRunner
            RealtimeSession
            Realtime Events
            Realtime Configuration
            Model
        Voice
            Pipeline
            Workflow
            Input
            Result
            Pipeline Config
            Events
            Exceptions
            Model
            Utils
            OpenAIVoiceModelProvider
            OpenAI STT
            OpenAI TTS
        Extensions
            Handoff filters
            Handoff prompt
            LiteLLM Models
            SQLAlchemySession
            EncryptedSession
            AdvancedSQLiteSession

Table of contents

    tool
    MCPToolApprovalFunction
    LocalShellExecutor
    ShellExecutor
    Tool
    ToolOutputText
    ToolOutputTextDict
    ToolOutputImage
        check_at_least_one_required_field
    ToolOutputImageDict
    ToolOutputFileContent
        check_at_least_one_required_field
    ToolOutputFileContentDict
    ComputerCreate
    ComputerDispose
    ComputerProvider
    FunctionToolResult
        tool
        output
        run_item
    FunctionTool
        name
        description
        params_json_schema
        on_invoke_tool
        strict_json_schema
        is_enabled
        tool_input_guardrails
        tool_output_guardrails
    FileSearchTool
        vector_store_ids
        max_num_results
        include_search_results
        ranking_options
        filters
    WebSearchTool
        user_location
        filters
        search_context_size
    ComputerTool
        computer
        on_safety_check
    ComputerToolSafetyCheckData
        ctx_wrapper
        agent
        tool_call
        safety_check
    MCPToolApprovalRequest
        ctx_wrapper
        data
    MCPToolApprovalFunctionResult
        approve
        reason
    HostedMCPTool
        tool_config
        on_approval_request
    CodeInterpreterTool
        tool_config
    ImageGenerationTool
        tool_config
    LocalShellCommandRequest
        ctx_wrapper
        data
    LocalShellTool
        executor
    ShellCallOutcome
    ShellCommandOutput
    ShellResult
    ShellActionRequest
    ShellCallData
    ShellCommandRequest
    ShellTool
    ApplyPatchTool
    resolve_computer
    dispose_resolved_computers
    default_tool_error_function
    function_tool

Tools
MCPToolApprovalFunction module-attribute

MCPToolApprovalFunction = Callable[
    
[MCPToolApprovalRequest],
    
MaybeAwaitable[MCPToolApprovalFunctionResult],
]

A function that approves or rejects a tool call.
LocalShellExecutor module-attribute

LocalShellExecutor = Callable[
    
[LocalShellCommandRequest], MaybeAwaitable[str]
]

A function that executes a command on a shell.
ShellExecutor module-attribute

ShellExecutor = Callable[
    
[ShellCommandRequest],
    
MaybeAwaitable[Union[str, ShellResult]],
]

Executes a shell command sequence and returns either text or structured output.
Tool module-attribute

Tool = Union[
    
FunctionTool,
    
FileSearchTool,
    
WebSearchTool,
    
ComputerTool[Any],
    
HostedMCPTool,
    
ShellTool,
    
ApplyPatchTool,
    
LocalShellTool,
    
ImageGenerationTool,
    
CodeInterpreterTool,
]

A tool that can be used in an agent.
ToolOutputText

Bases: BaseModel

Represents a tool output that should be sent to the model as text.
Source code in src/agents/tool.py

ToolOutputTextDict

Bases: TypedDict

TypedDict variant for text tool outputs.
Source code in src/agents/tool.py

ToolOutputImage

Bases: BaseModel

Represents a tool output that should be sent to the model as an image.

You can provide either an image_url (URL or data URL) or a file_id for previously uploaded content. The optional detail can control vision detail.
Source code in src/agents/tool.py

check_at_least_one_required_field

check_at_least_one_required_field() -> ToolOutputImage

Validate that at least one of image_url or file_id is provided.
Source code in src/agents/tool.py

ToolOutputImageDict

Bases: TypedDict

TypedDict variant for image tool outputs.
Source code in src/agents/tool.py

ToolOutputFileContent

Bases: BaseModel

Represents a tool output that should be sent to the model as a file.

Provide one of file_data (base64), file_url, or file_id. You may also provide an optional filename when using file_data to hint file name.
Source code in src/agents/tool.py

check_at_least_one_required_field

check_at_least_one_required_field() -> (
    
ToolOutputFileContent
)

Validate that at least one of file_data, file_url, or file_id is provided.
Source code in src/agents/tool.py

ToolOutputFileContentDict

Bases: TypedDict

TypedDict variant for file content tool outputs.
Source code in src/agents/tool.py

ComputerCreate

Bases: Protocol[ComputerT_co]

Initializes a computer for the current run context.
Source code in src/agents/tool.py

ComputerDispose

Bases: Protocol[ComputerT_contra]

Cleans up a computer initialized for a run context.
Source code in src/agents/tool.py

ComputerProvider dataclass

Bases: Generic[ComputerT]

Configures create/dispose hooks for per-run computer lifecycle management.
Source code in src/agents/tool.py

FunctionToolResult dataclass
Source code in src/agents/tool.py

tool instance-attribute

tool: FunctionTool

The tool that was run.
output instance-attribute

output: Any

The output of the tool.
run_item instance-attribute

run_item: RunItem

The run item that was produced as a result of the tool call.
FunctionTool dataclass

A tool that wraps a function. In most cases, you should use the function_tool helpers to create a FunctionTool, as they let you easily wrap a Python function.
Source code in src/agents/tool.py

name instance-attribute

name: str

The name of the tool, as shown to the LLM. Generally the name of the function.
description instance-attribute

description: str

A description of the tool, as shown to the LLM.
params_json_schema instance-attribute

params_json_schema: dict[str, Any]

The JSON schema for the tool's parameters.
on_invoke_tool instance-attribute

on_invoke_tool: Callable[
    
[ToolContext[Any], str], Awaitable[Any]
]

A function that invokes the tool with the given context and parameters. The params passed are: 1. The tool run context. 2. The arguments from the LLM, as a JSON string.

You must return a one of the structured tool output types (e.g. ToolOutputText, ToolOutputImage, ToolOutputFileContent) or a string representation of the tool output, or a list of them, or something we can call str() on. In case of errors, you can either raise an Exception (which will cause the run to fail) or return a string error message (which will be sent back to the LLM).
strict_json_schema class-attribute instance-attribute

strict_json_schema: bool = True

Whether the JSON schema is in strict mode. We strongly recommend setting this to True, as it increases the likelihood of correct JSON input.
is_enabled class-attribute instance-attribute

is_enabled: (
    
bool
    
| Callable[
        
[RunContextWrapper[Any], AgentBase],
        
MaybeAwaitable[bool],
    
]
) = True

Whether the tool is enabled. Either a bool or a Callable that takes the run context and agent and returns whether the tool is enabled. You can use this to dynamically enable/disable a tool based on your context/state.
tool_input_guardrails class-attribute instance-attribute

tool_input_guardrails: (
    
list[ToolInputGuardrail[Any]] | None
) = None

Optional list of input guardrails to run before invoking this tool.
tool_output_guardrails class-attribute instance-attribute

tool_output_guardrails: (
    
list[ToolOutputGuardrail[Any]] | None
) = None

Optional list of output guardrails to run after invoking this tool.
FileSearchTool dataclass

A hosted tool that lets the LLM search through a vector store. Currently only supported with OpenAI models, using the Responses API.
Source code in src/agents/tool.py

vector_store_ids instance-attribute

vector_store_ids: list[str]

The IDs of the vector stores to search.
max_num_results class-attribute instance-attribute

max_num_results: int | None = None

The maximum number of results to return.
include_search_results class-attribute instance-attribute

include_search_results: bool = False

Whether to include the search results in the output produced by the LLM.
ranking_options class-attribute instance-attribute

ranking_options: RankingOptions | None = None

Ranking options for search.
filters class-attribute instance-attribute

filters: Filters | None = None

A filter to apply based on file attributes.
WebSearchTool dataclass

A hosted tool that lets the LLM search the web. Currently only supported with OpenAI models, using the Responses API.
Source code in src/agents/tool.py

user_location class-attribute instance-attribute

user_location: UserLocation | None = None

Optional location for the search. Lets you customize results to be relevant to a location.
filters class-attribute instance-attribute

filters: Filters | None = None

A filter to apply based on file attributes.
search_context_size class-attribute instance-attribute

search_context_size: Literal["low", "medium", "high"] = (
    
"medium"
)

The amount of context to use for the search.
ComputerTool dataclass

Bases: Generic[ComputerT]

A hosted tool that lets the LLM control a computer.
Source code in src/agents/tool.py

computer instance-attribute

computer: ComputerConfig[ComputerT]

The computer implementation, or a factory that produces a computer per run.
on_safety_check class-attribute instance-attribute

on_safety_check: (
    
Callable[
        
[ComputerToolSafetyCheckData], MaybeAwaitable[bool]
    
]
    
| None
) = None

Optional callback to acknowledge computer tool safety checks.
ComputerToolSafetyCheckData dataclass

Information about a computer tool safety check.
Source code in src/agents/tool.py

ctx_wrapper instance-attribute

ctx_wrapper: RunContextWrapper[Any]

The run context.
agent instance-attribute

agent: Agent[Any]

The agent performing the computer action.
tool_call instance-attribute

tool_call: ResponseComputerToolCall

The computer tool call.
safety_check instance-attribute

safety_check: PendingSafetyCheck

The pending safety check to acknowledge.
MCPToolApprovalRequest dataclass

A request to approve a tool call.
Source code in src/agents/tool.py

ctx_wrapper instance-attribute

ctx_wrapper: RunContextWrapper[Any]

The run context.
data instance-attribute

data: McpApprovalRequest

The data from the MCP tool approval request.
MCPToolApprovalFunctionResult

Bases: TypedDict

The result of an MCP tool approval function.
Source code in src/agents/tool.py

approve instance-attribute

approve: bool

Whether to approve the tool call.
reason instance-attribute

reason: NotRequired[str]

An optional reason, if rejected.
HostedMCPTool dataclass

A tool that allows the LLM to use a remote MCP server. The LLM will automatically list and call tools, without requiring a round trip back to your code. If you want to run MCP servers locally via stdio, in a VPC or other non-publicly-accessible environment, or you just prefer to run tool calls locally, then you can instead use the servers in agents.mcp and pass Agent(mcp_servers=[...]) to the agent.
Source code in src/agents/tool.py

tool_config instance-attribute

tool_config: Mcp

The MCP tool config, which includes the server URL and other settings.
on_approval_request class-attribute instance-attribute

on_approval_request: MCPToolApprovalFunction | None = None

An optional function that will be called if approval is requested for an MCP tool. If not provided, you will need to manually add approvals/rejections to the input and call Runner.run(...) again.
CodeInterpreterTool dataclass

A tool that allows the LLM to execute code in a sandboxed environment.
Source code in src/agents/tool.py

tool_config instance-attribute

tool_config: CodeInterpreter

The tool config, which includes the container and other settings.
ImageGenerationTool dataclass

A tool that allows the LLM to generate images.
Source code in src/agents/tool.py

tool_config instance-attribute

tool_config: ImageGeneration

The tool config, which image generation settings.
LocalShellCommandRequest dataclass

A request to execute a command on a shell.
Source code in src/agents/tool.py

ctx_wrapper instance-attribute

ctx_wrapper: RunContextWrapper[Any]

The run context.
data instance-attribute

data: LocalShellCall

The data from the local shell tool call.
LocalShellTool dataclass

A tool that allows the LLM to execute commands on a shell.

For more details, see: https://platform.openai.com/docs/guides/tools-local-shell
Source code in src/agents/tool.py

executor instance-attribute

executor: LocalShellExecutor

A function that executes a command on a shell.
ShellCallOutcome dataclass

Describes the terminal condition of a shell command.
Source code in src/agents/tool.py

ShellCommandOutput dataclass

Structured output for a single shell command execution.
Source code in src/agents/tool.py

ShellResult dataclass

Result returned by a shell executor.
Source code in src/agents/tool.py

ShellActionRequest dataclass

Action payload for a next-generation shell call.
Source code in src/agents/tool.py

ShellCallData dataclass

Normalized shell call data provided to shell executors.
Source code in src/agents/tool.py

ShellCommandRequest dataclass

A request to execute a modern shell call.
Source code in src/agents/tool.py

ShellTool dataclass

Next-generation shell tool. LocalShellTool will be deprecated in favor of this.
Source code in src/agents/tool.py

ApplyPatchTool dataclass

Hosted apply_patch tool. Lets the model request file mutations via unified diffs.
Source code in src/agents/tool.py

resolve_computer async

resolve_computer(
    
*,
    
tool: ComputerTool[Any],
    
run_context: RunContextWrapper[Any],
) -> ComputerLike

Resolve a computer for a given run context, initializing it if needed.
Source code in src/agents/tool.py

dispose_resolved_computers async

dispose_resolved_computers(
    
*, run_context: RunContextWrapper[Any]
) -> None

Dispose any computer instances created for the provided run context.
Source code in src/agents/tool.py

default_tool_error_function

default_tool_error_function(
    
ctx: RunContextWrapper[Any], error: Exception
) -> str

The default tool error function, which just returns a generic error message.
Source code in src/agents/tool.py

function_tool

function_tool(
    
func: ToolFunction[...],
    
*,
    
name_override: str | None = None,
    
description_override: str | None = None,
    
docstring_style: DocstringStyle | None = None,
    
use_docstring_info: bool = True,
    
failure_error_function: ToolErrorFunction | None = None,
    
strict_mode: bool = True,
    
is_enabled: bool
    
| Callable[
        
[RunContextWrapper[Any], AgentBase],
        
MaybeAwaitable[bool],
    
] = True,
    
tool_input_guardrails: list[ToolInputGuardrail[Any]]
    
| None = None,
    
tool_output_guardrails: list[ToolOutputGuardrail[Any]]
    
| None = None,
) -> FunctionTool

function_tool(
    
*,
    
name_override: str | None = None,
    
description_override: str | None = None,
    
docstring_style: DocstringStyle | None = None,
    
use_docstring_info: bool = True,
    
failure_error_function: ToolErrorFunction | None = None,
    
strict_mode: bool = True,
    
is_enabled: bool
    
| Callable[
        
[RunContextWrapper[Any], AgentBase],
        
MaybeAwaitable[bool],
    
] = True,
    
tool_input_guardrails: list[ToolInputGuardrail[Any]]
    
| None = None,
    
tool_output_guardrails: list[ToolOutputGuardrail[Any]]
    
| None = None,
) -> Callable[[ToolFunction[...]], FunctionTool]

function_tool(
    
func: ToolFunction[...] | None = None,
    
*,
    
name_override: str | None = None,
    
description_override: str | None = None,
    
docstring_style: DocstringStyle | None = None,
    
use_docstring_info: bool = True,
    
failure_error_function: ToolErrorFunction
    
| None = default_tool_error_function,
    
strict_mode: bool = True,
    
is_enabled: bool
    
| Callable[
        
[RunContextWrapper[Any], AgentBase],
        
MaybeAwaitable[bool],
    
] = True,
    
tool_input_guardrails: list[ToolInputGuardrail[Any]]
    
| None = None,
    
tool_output_guardrails: list[ToolOutputGuardrail[Any]]
    
| None = None,
) -> (
    
FunctionTool
    
| Callable[[ToolFunction[...]], FunctionTool]
)

Decorator to create a FunctionTool from a function. By default, we will: 1. Parse the function signature to create a JSON schema for the tool's parameters. 2. Use the function's docstring to populate the tool's description. 3. Use the function's docstring to populate argument descriptions. The docstring style is detected automatically, but you can override it.

If the function takes a RunContextWrapper as the first argument, it must match the context type of the agent that uses the tool.

Parameters:
Name 	Type 	Description 	Default
func 	ToolFunction[...] | None 	

The function to wrap.
	None
name_override 	str | None 	

If provided, use this name for the tool instead of the function's name.
	None
description_override 	str | None 	

If provided, use this description for the tool instead of the function's docstring.
	None
docstring_style 	DocstringStyle | None 	

If provided, use this style for the tool's docstring. If not provided, we will attempt to auto-detect the style.
	None
use_docstring_info 	bool 	

If True, use the function's docstring to populate the tool's description and argument descriptions.
	True
failure_error_function 	ToolErrorFunction | None 	

If provided, use this function to generate an error message when the tool call fails. The error message is sent to the LLM. If you pass None, then no error message will be sent and instead an Exception will be raised.
	default_tool_error_function
strict_mode 	bool 	

Whether to enable strict mode for the tool's JSON schema. We strongly recommend setting this to True, as it increases the likelihood of correct JSON input. If False, it allows non-strict JSON schemas. For example, if a parameter has a default value, it will be optional, additional properties are allowed, etc. See here for more: https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses#supported-schemas
	True
is_enabled 	bool | Callable[[RunContextWrapper[Any], AgentBase], MaybeAwaitable[bool]] 	

Whether the tool is enabled. Can be a bool or a callable that takes the run context and agent and returns whether the tool is enabled. Disabled tools are hidden from the LLM at runtime.
	True
tool_input_guardrails 	list[ToolInputGuardrail[Any]] | None 	

Optional list of guardrails to run before invoking the tool.
	None
tool_output_guardrails 	list[ToolOutputGuardrail[Any]] | None 	

Optional list of guardrails to run after the tool returns.
	None
Source code in src/agents/tool.py

